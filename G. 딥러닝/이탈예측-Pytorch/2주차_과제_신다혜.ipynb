{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2주차\n",
    "\n",
    "### 선형회귀모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [2.],\n",
       "         [3.]]),\n",
       " torch.Size([3, 1]),\n",
       " tensor([[2.],\n",
       "         [4.],\n",
       "         [6.]]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_train.shape,y_train, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad =True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = W*x_train + b\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((y_train-y_hat)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W,b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch: 100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch: 200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch: 300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch: 400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch: 500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch: 600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch: 700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch: 800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch: 900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch:1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch:1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch:1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch:1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch:1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch:1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch:1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch:1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch:1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch:1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터 \n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad= True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# Optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr = 0.01)\n",
    "\n",
    "nb_epochs = 1999 #반복수\n",
    "for epoch in range(nb_epochs +1):\n",
    "    # H(x) 계산\n",
    "    y_hat = x_train*W + b\n",
    "    # cost 계산\n",
    "    cost = torch.mean((y_hat - y_train)**2)\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 ==0:\n",
    "        print(\"Epoch:{:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}\".format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = nn.Linear(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = F.mse_loss(y_hat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.1863]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4795], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost:0.000001\n",
      "Epoch  100/2000 Cost:0.000000\n",
      "Epoch  200/2000 Cost:0.000000\n",
      "Epoch  300/2000 Cost:0.000000\n",
      "Epoch  400/2000 Cost:0.000000\n",
      "Epoch  500/2000 Cost:0.000000\n",
      "Epoch  600/2000 Cost:0.000000\n",
      "Epoch  700/2000 Cost:0.000000\n",
      "Epoch  800/2000 Cost:0.000000\n",
      "Epoch  900/2000 Cost:0.000000\n",
      "Epoch 1000/2000 Cost:0.000000\n",
      "Epoch 1100/2000 Cost:0.000000\n",
      "Epoch 1200/2000 Cost:0.000000\n",
      "Epoch 1300/2000 Cost:0.000000\n",
      "Epoch 1400/2000 Cost:0.000000\n",
      "Epoch 1500/2000 Cost:0.000000\n",
      "Epoch 1600/2000 Cost:0.000000\n",
      "Epoch 1700/2000 Cost:0.000000\n",
      "Epoch 1800/2000 Cost:0.000000\n",
      "Epoch 1900/2000 Cost:0.000000\n",
      "Epoch 2000/2000 Cost:0.000000\n",
      "[Parameter containing:\n",
      "tensor([[2.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([2.0421e-05], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    #cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    #cost로 H(x)개선하는 부분\n",
    "    #gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    #비용함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if epoch %100 == 0:\n",
    "        # 100번마다 로그 출력\n",
    "        print(\"Epoch {:4d}/{} Cost:{:.6f}\".format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        )\n",
    "              )\n",
    "\n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2,1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.770649 Accuracy: 50.00%\n",
      "Epoch   20/1000 Cost: 0.520903 Accuracy: 83.33%\n",
      "Epoch   40/1000 Cost: 0.346186 Accuracy: 83.33%\n",
      "Epoch   60/1000 Cost: 0.205374 Accuracy: 83.33%\n",
      "Epoch   80/1000 Cost: 0.148449 Accuracy: 100.00%\n",
      "Epoch  100/1000 Cost: 0.128891 Accuracy: 100.00%\n",
      "Epoch  120/1000 Cost: 0.114122 Accuracy: 100.00%\n",
      "Epoch  140/1000 Cost: 0.102445 Accuracy: 100.00%\n",
      "Epoch  160/1000 Cost: 0.092980 Accuracy: 100.00%\n",
      "Epoch  180/1000 Cost: 0.085152 Accuracy: 100.00%\n",
      "Epoch  200/1000 Cost: 0.078568 Accuracy: 100.00%\n",
      "Epoch  220/1000 Cost: 0.072952 Accuracy: 100.00%\n",
      "Epoch  240/1000 Cost: 0.068102 Accuracy: 100.00%\n",
      "Epoch  260/1000 Cost: 0.063871 Accuracy: 100.00%\n",
      "Epoch  280/1000 Cost: 0.060146 Accuracy: 100.00%\n",
      "Epoch  300/1000 Cost: 0.056841 Accuracy: 100.00%\n",
      "Epoch  320/1000 Cost: 0.053887 Accuracy: 100.00%\n",
      "Epoch  340/1000 Cost: 0.051231 Accuracy: 100.00%\n",
      "Epoch  360/1000 Cost: 0.048830 Accuracy: 100.00%\n",
      "Epoch  380/1000 Cost: 0.046648 Accuracy: 100.00%\n",
      "Epoch  400/1000 Cost: 0.044656 Accuracy: 100.00%\n",
      "Epoch  420/1000 Cost: 0.042829 Accuracy: 100.00%\n",
      "Epoch  440/1000 Cost: 0.041149 Accuracy: 100.00%\n",
      "Epoch  460/1000 Cost: 0.039598 Accuracy: 100.00%\n",
      "Epoch  480/1000 Cost: 0.038162 Accuracy: 100.00%\n",
      "Epoch  500/1000 Cost: 0.036827 Accuracy: 100.00%\n",
      "Epoch  520/1000 Cost: 0.035584 Accuracy: 100.00%\n",
      "Epoch  540/1000 Cost: 0.034423 Accuracy: 100.00%\n",
      "Epoch  560/1000 Cost: 0.033337 Accuracy: 100.00%\n",
      "Epoch  580/1000 Cost: 0.032318 Accuracy: 100.00%\n",
      "Epoch  600/1000 Cost: 0.031360 Accuracy: 100.00%\n",
      "Epoch  620/1000 Cost: 0.030458 Accuracy: 100.00%\n",
      "Epoch  640/1000 Cost: 0.029607 Accuracy: 100.00%\n",
      "Epoch  660/1000 Cost: 0.028803 Accuracy: 100.00%\n",
      "Epoch  680/1000 Cost: 0.028041 Accuracy: 100.00%\n",
      "Epoch  700/1000 Cost: 0.027320 Accuracy: 100.00%\n",
      "Epoch  720/1000 Cost: 0.026635 Accuracy: 100.00%\n",
      "Epoch  740/1000 Cost: 0.025983 Accuracy: 100.00%\n",
      "Epoch  760/1000 Cost: 0.025364 Accuracy: 100.00%\n",
      "Epoch  780/1000 Cost: 0.024773 Accuracy: 100.00%\n",
      "Epoch  800/1000 Cost: 0.024209 Accuracy: 100.00%\n",
      "Epoch  820/1000 Cost: 0.023671 Accuracy: 100.00%\n",
      "Epoch  840/1000 Cost: 0.023157 Accuracy: 100.00%\n",
      "Epoch  860/1000 Cost: 0.022664 Accuracy: 100.00%\n",
      "Epoch  880/1000 Cost: 0.022192 Accuracy: 100.00%\n",
      "Epoch  900/1000 Cost: 0.021740 Accuracy: 100.00%\n",
      "Epoch  920/1000 Cost: 0.021305 Accuracy: 100.00%\n",
      "Epoch  940/1000 Cost: 0.020888 Accuracy: 100.00%\n",
      "Epoch  960/1000 Cost: 0.020487 Accuracy: 100.00%\n",
      "Epoch  980/1000 Cost: 0.020101 Accuracy: 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019730 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    #H(x)계산\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    #Cost계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    #Cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #20번마다 로그 출력\n",
    "    if epoch%20 ==0:\n",
    "        # 예측값이 0.5 넘으면 True\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        # 실제값과 일치하는 경우만 True\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        # 정확도 계산\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        \n",
    "        print(\"Epoch {:4d}/{} Cost: {:.6f} Accuracy: {:2.2f}%\".format(\n",
    "            epoch, nb_epochs, cost.item(), accuracy*100\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0/1000 Cost: 0.636346 Accuracy 50.00%\n",
      "Epoch  20/1000 Cost: 0.380800 Accuracy 83.33%\n",
      "Epoch  40/1000 Cost: 0.271243 Accuracy 83.33%\n",
      "Epoch  60/1000 Cost: 0.185655 Accuracy 100.00%\n",
      "Epoch  80/1000 Cost: 0.145254 Accuracy 100.00%\n",
      "Epoch 100/1000 Cost: 0.126625 Accuracy 100.00%\n",
      "Epoch 120/1000 Cost: 0.112351 Accuracy 100.00%\n",
      "Epoch 140/1000 Cost: 0.101023 Accuracy 100.00%\n",
      "Epoch 160/1000 Cost: 0.091813 Accuracy 100.00%\n",
      "Epoch 180/1000 Cost: 0.084177 Accuracy 100.00%\n",
      "Epoch 200/1000 Cost: 0.077741 Accuracy 100.00%\n",
      "Epoch 220/1000 Cost: 0.072241 Accuracy 100.00%\n",
      "Epoch 240/1000 Cost: 0.067484 Accuracy 100.00%\n",
      "Epoch 260/1000 Cost: 0.063329 Accuracy 100.00%\n",
      "Epoch 280/1000 Cost: 0.059667 Accuracy 100.00%\n",
      "Epoch 300/1000 Cost: 0.056414 Accuracy 100.00%\n",
      "Epoch 320/1000 Cost: 0.053504 Accuracy 100.00%\n",
      "Epoch 340/1000 Cost: 0.050886 Accuracy 100.00%\n",
      "Epoch 360/1000 Cost: 0.048516 Accuracy 100.00%\n",
      "Epoch 380/1000 Cost: 0.046362 Accuracy 100.00%\n",
      "Epoch 400/1000 Cost: 0.044394 Accuracy 100.00%\n",
      "Epoch 420/1000 Cost: 0.042589 Accuracy 100.00%\n",
      "Epoch 440/1000 Cost: 0.040928 Accuracy 100.00%\n",
      "Epoch 460/1000 Cost: 0.039393 Accuracy 100.00%\n",
      "Epoch 480/1000 Cost: 0.037972 Accuracy 100.00%\n",
      "Epoch 500/1000 Cost: 0.036650 Accuracy 100.00%\n",
      "Epoch 520/1000 Cost: 0.035419 Accuracy 100.00%\n",
      "Epoch 540/1000 Cost: 0.034269 Accuracy 100.00%\n",
      "Epoch 560/1000 Cost: 0.033192 Accuracy 100.00%\n",
      "Epoch 580/1000 Cost: 0.032182 Accuracy 100.00%\n",
      "Epoch 600/1000 Cost: 0.031232 Accuracy 100.00%\n",
      "Epoch 620/1000 Cost: 0.030337 Accuracy 100.00%\n",
      "Epoch 640/1000 Cost: 0.029493 Accuracy 100.00%\n",
      "Epoch 660/1000 Cost: 0.028695 Accuracy 100.00%\n",
      "Epoch 680/1000 Cost: 0.027939 Accuracy 100.00%\n",
      "Epoch 700/1000 Cost: 0.027223 Accuracy 100.00%\n",
      "Epoch 720/1000 Cost: 0.026543 Accuracy 100.00%\n",
      "Epoch 740/1000 Cost: 0.025896 Accuracy 100.00%\n",
      "Epoch 760/1000 Cost: 0.025280 Accuracy 100.00%\n",
      "Epoch 780/1000 Cost: 0.024694 Accuracy 100.00%\n",
      "Epoch 800/1000 Cost: 0.024134 Accuracy 100.00%\n",
      "Epoch 820/1000 Cost: 0.023599 Accuracy 100.00%\n",
      "Epoch 840/1000 Cost: 0.023087 Accuracy 100.00%\n",
      "Epoch 860/1000 Cost: 0.022598 Accuracy 100.00%\n",
      "Epoch 880/1000 Cost: 0.022129 Accuracy 100.00%\n",
      "Epoch 900/1000 Cost: 0.021679 Accuracy 100.00%\n",
      "Epoch 920/1000 Cost: 0.021247 Accuracy 100.00%\n",
      "Epoch 940/1000 Cost: 0.020832 Accuracy 100.00%\n",
      "Epoch 960/1000 Cost: 0.020433 Accuracy 100.00%\n",
      "Epoch 980/1000 Cost: 0.020049 Accuracy 100.00%\n",
      "Epoch1000/1000 Cost: 0.019680 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    #H(x)계산\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    #cost계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #20번마다 로그 출력\n",
    "    if epoch % 20 == 0:\n",
    "        prediction = hypothesis >=torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train \n",
    "        accuracy = correct_prediction.sum().item()/len(correct_prediction)   \n",
    "        print(\"Epoch{:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%\".format(\n",
    "            epoch, nb_epochs, cost.item(), accuracy*100\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8,3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 2.606828\n",
      "Epoch   20/1000 Cost: 1.027093\n",
      "Epoch   40/1000 Cost: 0.876387\n",
      "Epoch   60/1000 Cost: 0.802816\n",
      "Epoch   80/1000 Cost: 0.759394\n",
      "Epoch  100/1000 Cost: 0.729634\n",
      "Epoch  120/1000 Cost: 0.706820\n",
      "Epoch  140/1000 Cost: 0.687958\n",
      "Epoch  160/1000 Cost: 0.671568\n",
      "Epoch  180/1000 Cost: 0.656838\n",
      "Epoch  200/1000 Cost: 0.643285\n",
      "Epoch  220/1000 Cost: 0.630599\n",
      "Epoch  240/1000 Cost: 0.618574\n",
      "Epoch  260/1000 Cost: 0.607064\n",
      "Epoch  280/1000 Cost: 0.595964\n",
      "Epoch  300/1000 Cost: 0.585195\n",
      "Epoch  320/1000 Cost: 0.574697\n",
      "Epoch  340/1000 Cost: 0.564424\n",
      "Epoch  360/1000 Cost: 0.554338\n",
      "Epoch  380/1000 Cost: 0.544411\n",
      "Epoch  400/1000 Cost: 0.534617\n",
      "Epoch  420/1000 Cost: 0.524938\n",
      "Epoch  440/1000 Cost: 0.515357\n",
      "Epoch  460/1000 Cost: 0.505859\n",
      "Epoch  480/1000 Cost: 0.496432\n",
      "Epoch  500/1000 Cost: 0.487067\n",
      "Epoch  520/1000 Cost: 0.477755\n",
      "Epoch  540/1000 Cost: 0.468488\n",
      "Epoch  560/1000 Cost: 0.459259\n",
      "Epoch  580/1000 Cost: 0.450062\n",
      "Epoch  600/1000 Cost: 0.440892\n",
      "Epoch  620/1000 Cost: 0.431745\n",
      "Epoch  640/1000 Cost: 0.422615\n",
      "Epoch  660/1000 Cost: 0.413500\n",
      "Epoch  680/1000 Cost: 0.404396\n",
      "Epoch  700/1000 Cost: 0.395300\n",
      "Epoch  720/1000 Cost: 0.386212\n",
      "Epoch  740/1000 Cost: 0.377129\n",
      "Epoch  760/1000 Cost: 0.368050\n",
      "Epoch  780/1000 Cost: 0.358976\n",
      "Epoch  800/1000 Cost: 0.349909\n",
      "Epoch  820/1000 Cost: 0.340852\n",
      "Epoch  840/1000 Cost: 0.331809\n",
      "Epoch  860/1000 Cost: 0.322788\n",
      "Epoch  880/1000 Cost: 0.313803\n",
      "Epoch  900/1000 Cost: 0.304871\n",
      "Epoch  920/1000 Cost: 0.296022\n",
      "Epoch  940/1000 Cost: 0.287298\n",
      "Epoch  960/1000 Cost: 0.278768\n",
      "Epoch  980/1000 Cost: 0.270541\n",
      "Epoch 1000/1000 Cost: 0.262787\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 20== 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myAnaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
