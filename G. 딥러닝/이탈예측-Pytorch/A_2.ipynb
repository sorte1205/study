{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm :  벡터의 크기\n",
    "\n",
    "정의. $R^n$의 벡터 $ x= (x_1, x_2, ... , x_n)$에 대하여\n",
    "\n",
    "$ \\| \\mathbf{x} \\| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "### 벡터의 내적 \n",
    "\n",
    "정의. \n",
    "$(\\mathbf{x}, \\mathbf{y})= x_1y_2+x_2y_2  + ... + x_ny_n$\n",
    "\n",
    "\n",
    "\n",
    "### 벡터의 유사도\n",
    "\n",
    "$ \\| \\mathbf{x-y} \\| = \\sqrt{(x_1^2 -y_1^2)^2+ (x_2^2 -y_2^2)^2 + \\cdots +(x_n^2 -y_n^2)^2}\n",
    "$\n",
    "\n",
    "\n",
    "### 코사인 유사도\n",
    "\n",
    "$\\text{cosine similarity}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\| \\mathbf{u} \\| \\| \\mathbf{v} \\|}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 노름(Norm) 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2361)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([1,2])\n",
    "\n",
    "torch.norm(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(math.pow(1,2)+ math.pow(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코사인 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = torch.FloatTensor([1,1])\n",
    "vec2 = torch.FloatTensor([2,2])\n",
    "cos = nn.CosineSimilarity(dim = 0)\n",
    "loss = cos(vec1, vec2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "> Least Squared Method (최소제곱법, 최소자승법)\n",
    ">\n",
    ">MSE= $ \\frac {1}{N} \\displaystyle\\sum^N_{i=1}(y_i- \\hat{y_i}^2) $\n",
    ">\n",
    "> 고르게 오차가 생기는 것을 지향,. 한 가지 오차가 큰 것에 패널티를 주도록 한다.\n",
    ">\n",
    "> \n",
    "> 손실함수: 잔차의 제곱\n",
    "> \n",
    "> 비용함수: 잔차 제곱의 합\n",
    "> \n",
    "> 목적함수: 잔체 제곱의 합을 최소로 함\n",
    "\n",
    "\n",
    " 미분을 하기?\n",
    " \n",
    ">Optimizer : loss가 최소값인 지점을 찾는다.\n",
    ">\n",
    ">$ y = W_x + b$\n",
    ">- Gradient = $ \\frac {\\delta loss}{\\delta W}$\n",
    ">\n",
    ">- $ W_{new} = W - \\alpha \\frac {\\delta loss}{\\delta W}$\n",
    ">\n",
    ">- $\\alpha$ 는 학습률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "> 독립 변수의 선형 결합으로 종속 변수를 설명\n",
    ">\n",
    "> 종속 변수가 범주형 데이터\n",
    ">\n",
    "> 각 독립변수를 입력으로 받아서 종속 변수의 특정 클래스로 분류하는 것이 목적\n",
    ">\n",
    "> 입력 값의 범위 $(-\\infty ~ +\\infty)$\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Odds Ratio 는 성공 확률이 실패 확률의 몇 배인지를 나타내는 값\n",
    "  - (성공, 실패) 확률이 (p(x), 1-p(x))\n",
    "  - Odds Ratio = $\\frac {p(x) }{1-p(x)}$\n",
    "\n",
    "  - 일반적으로 Odds에 로그를 취한 Log-Odds 사용\n",
    "    - 확률 $p_+ (x)$에 대한 Log-Odds f(x)\n",
    "\n",
    "BCE (Binary Cross Entropy) = $\\frac{1}{N}\\displaystyle\\sum^N_{i=0}y_i * log(\\hat{y}_i) + (1-y_i) * log(1-\\hat{y}_i)$\n",
    "\n",
    "> $\\hat{y}$ : 예측값, y: 실제값\n",
    ">\n",
    "> $cost(W) = -\\displaystyle\\sum^k_{j=1}y_jlog(p_j)$     $\\space\\space$ * softmax의 비용함수도 이와 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression (1,2,3)\n",
    "\n",
    "\n",
    "$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}$    $\\space\\space$ \n",
    "\n",
    "softmax 비용함수\n",
    "\n",
    "> $cost(W) = -\\displaystyle\\sum^n_{i=1}\\displaystyle\\sum^k_{j=1}y_j^{(i)}log(p_j^{(i)})$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선형회귀모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn               #딥러닝 관련 모듈\n",
    "import torch.optim as optim     \n",
    "import torch.nn.functional as F #손실함수 관련 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [2.],\n",
       "         [3.]]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.],\n",
       "         [4.],\n",
       "         [6.]]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True)                  # W 파라미터 만들기. 0이 하나 들어있다. gradient descent 쓸것.\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True)                  # b 파라미터 만들기. 0이 하나 들어있다. gradient descent 쓸것.\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = W*x_train + b\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y 와 $\\hat{y}$의 차이(오차) 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((y_train - y_hat)**2)   #**2는 제곱\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD([W,b], lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m)     \u001b[38;5;66;03m# lr은 학습률 = alpha\u001b[39;00m\n\u001b[0;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()    \u001b[38;5;66;03m#optimizer 초기화\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m              \u001b[38;5;66;03m# W의 업데이트를 위한 코드\u001b[39;00m\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([W,b], lr = 0.01)     # lr은 학습률 = alpha\n",
    "\n",
    "optimizer.zero_grad()    #optimizer 초기화\n",
    "cost.backward()              # W의 업데이트를 위한 코드\n",
    "optimizer.step()            # W(가중치) 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    y_hat = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((y_hat - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = nn.Linear(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = F.mse_loss(y_hat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.3019]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3271], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1999 W: 1.997, b: 0.006 Cost: 0.000005\n",
      "Epoch  100/1999 W: 1.998, b: 0.005 Cost: 0.000003\n",
      "Epoch  200/1999 W: 1.998, b: 0.004 Cost: 0.000002\n",
      "Epoch  300/1999 W: 1.999, b: 0.003 Cost: 0.000001\n",
      "Epoch  400/1999 W: 1.999, b: 0.002 Cost: 0.000001\n",
      "Epoch  500/1999 W: 1.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  600/1999 W: 1.999, b: 0.001 Cost: 0.000000\n",
      "Epoch  700/1999 W: 2.000, b: 0.001 Cost: 0.000000\n",
      "Epoch  800/1999 W: 2.000, b: 0.001 Cost: 0.000000\n",
      "Epoch  900/1999 W: 2.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1000/1999 W: 2.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1100/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1200/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1300/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1400/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1500/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1600/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1700/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1800/1999 W: 2.000, b: 0.000 Cost: 0.000000\n",
      "Epoch 1900/1999 W: 2.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "# W = torch.zeros(1, requires_grad=True)\n",
    "# b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    y_hat = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((y_hat - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로지스틱 회귀모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0],[0],[0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2,1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1778],\n",
       "        [0.0888],\n",
       "        [0.1038],\n",
       "        [0.0364],\n",
       "        [0.0230],\n",
       "        [0.0198]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train)   # 각각이 1로 분류될 수 있는 확률."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_hat >0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   10/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   20/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   30/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   40/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   50/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   60/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   70/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   80/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch   90/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  100/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  110/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  120/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  130/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  140/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  150/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  160/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  170/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  180/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  190/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  200/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  210/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  220/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  230/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  240/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  250/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  260/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  270/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  280/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  290/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  300/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  310/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  320/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  330/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  340/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  350/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  360/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  370/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  380/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  390/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  400/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  410/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  420/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  430/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  440/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  450/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  460/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  470/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  480/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  490/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  500/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  510/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  520/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  530/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  540/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  550/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  560/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  570/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  580/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  590/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  600/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  610/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  620/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  630/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  640/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  650/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  660/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  670/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  680/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  690/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  700/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  710/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  720/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  730/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  740/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  750/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  760/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  770/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  780/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  790/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  800/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  810/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  820/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  830/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  840/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  850/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  860/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  870/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  880/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  890/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  900/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  910/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  920/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  930/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  940/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  950/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  960/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  970/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  980/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch  990/1000 Cost: 1.901183, Accuracy: 50.00\n",
      "Epoch 1000/1000 Cost: 1.901183, Accuracy: 50.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    y_hat = x_train * W + b\n",
    "    hypothesis = model(x_train)\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}, Accuracy: {:2.2f}'.format(\n",
    "            epoch, nb_epochs,cost.item(), accuracy*100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   10/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   20/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   30/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   40/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   50/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   60/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   70/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   80/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch   90/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  100/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  110/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  120/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  130/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  140/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  150/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  160/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  170/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  180/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  190/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  200/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  210/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  220/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  230/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  240/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  250/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  260/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  270/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  280/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  290/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  300/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  310/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  320/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  330/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  340/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  350/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  360/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  370/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  380/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  390/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  400/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  410/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  420/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  430/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  440/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  450/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  460/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  470/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  480/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  490/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  500/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  510/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  520/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  530/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  540/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  550/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  560/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  570/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  580/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  590/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  600/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  610/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  620/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  630/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  640/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  650/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  660/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  670/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  680/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  690/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  700/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  710/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  720/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  730/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  740/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  750/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  760/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  770/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  780/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  790/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  800/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  810/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  820/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  830/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  840/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  850/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  860/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  870/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  880/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  890/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  900/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  910/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  920/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  930/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  940/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  950/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  960/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  970/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  980/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch  990/1000 Cost: 1.025021, Accuracy: 33.33\n",
      "Epoch 1000/1000 Cost: 1.025021, Accuracy: 33.33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    y_hat = x_train * W + b\n",
    "    hypothesis = model(x_train)\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}, Accuracy: {:2.2f}'.format(\n",
    "            epoch, nb_epochs,cost.item(), accuracy*100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x4 and 2x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m nb_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;66;03m# 원하는만큼 경사 하강법을 반복\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# H(x) 계산\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# cost 계산\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     cost \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(prediction, y_train)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 8\u001b[0m, in \u001b[0;36mBinaryClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\myAnaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x4 and 2x1)"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "\n",
    "    prediction = model(x_train)\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myAnaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
